# ScalableRobustGP
Implementation of the Machine Learning model contributed in the paper

[Predicting product quality in continuous manufacturing processes using a scalable robust Gaussian Process approach](https://pdf.sciencedirectassets.com/271095/1-s2.0-S0952197623X00141/1-s2.0-S0952197623014173/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDoaCXVzLWVhc3QtMSJHMEUCIHHFoXVN0H5gDGyu%2B3XV0SYoUb2fnJx6wI8GEZAuC4qOAiEA1G4TzbaE4TaLBopnzVV%2B5V2scYYUNIFAU4PK6EYmfpcqswUIMhAFGgwwNTkwMDM1NDY4NjUiDJnjDhIj1srtWMJsPSqQBeYHO1WDu9tpJX6zUH7PpGvk%2BmCsGCSgDVpwV8FBZ8Dsju5pK9bMrZXv5gF4IrEj0US0gJ9K814oT8qTGVGjAYAch8I8SSm4edVHDh71Vy5DDGrDCbceRBXZ%2BNb3eDHuZOcruM2fVhMhtYPOIbgh38tQYkDJu5ezsIZjArGT3c14Nz6x6fEBHezDyh12Xk2i1shT7gNwiqdjOTQBHbZDrpIMisDY1GWhIZpnZ5gZ%2BLqWQ%2FV8UMOLwRFFZWFxQ%2FcpsCvqcVAX9h65oHXTZ1HkE13BHUIbKeZg9sLa14u55c1Ps7puODrxIvz0u6hueb4ag14vKduMS%2FanV6E9IDKDqrv%2FU1frm97AG7adc2VjyKw%2FCb9iDifL1KQwn%2F8yIhByCjAzy1W4U3PqHSM8MibFb%2FwwlUuXnu7jmXauHt%2FaXJgA9O7QhqAVJgOelUwlzJ7o8T7Ka4s%2FrN1P4JT%2BzHlLj5ae3HNp8I5nWbtNHMVfCQ98rr%2Fhc3GaMR4bYXWOA7sFW8jJG5cEitv5ffRvRGiFGnOdK8WBdLzZx4JqT2Cqg8PrDyUFWwBPtMTi9cjpoJKStna23B5qdst2sIa93K4Io4E4ijZ8H4iYbiLPyrutXaJClpyMQUAWb8QAwMBXNmPOv5Vdjqg9acAzgXgd2L%2FYP2Q0es10s0ZUAOeMU3nE92lkpfrtVvdmCOa6WMgsT%2Bp5z31Y1GWVhCcVd%2FPI668x3NDI1c8P9RvGFc2qAVi%2F5UIb3zZnltjl4O%2BjjSGSeXHkrQZRRDRx18Y5Yo2iXJTP3ogyUNnM21HiRokys%2BD9HLpSsFdOvY9RgqL1UBh6EvQjQfEVaHPrGsSUtM1jkvI1ctX7jYBWPffjyUNf5G6XfcmAMJ%2BciK8GOrEBSOnjJ4CEuT4f95TniLxz8urNyaNvYk6ZU2LU0AIzWwNxRYs80Ctraeew5EnltYQOffdWDfVa8%2Bs9RNDKcqJNoC9gmRa0owJSEdkzaYTR6XlLBWu%2Fp07SVNBDhH5%2BaHcozumqdvkr%2FFQcpFvflH1PVR82360Velm3mjFPHIQ2QAkK%2F%2FNspATfJ5VWQ18T%2FIERSocqJwhdTHRoloT9eFYnGzHh062EtZtS7XTs1aU3zErZ&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240301T172212Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY52V6WBOF%2F20240301%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=cc997cab55c014e046b4025c07e5d1454c7946999f9b88c06bc568a02672c88c&hash=f8149a2ee2f540b33a54018eda25547590cd005a28ff3b1edd5d508d2591c780&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0952197623014173&tid=spdf-197986cb-b02b-4563-b3f5-9521cd107bbd&sid=e775532e4af775468119e0802ac9152cd7bagxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1d005a510b0107500304&rr=85dad3ec9aa47726&cc=gb)

The model uses a Bayesian Nonparametric approach based on the iterative application of Dirichlet Process (DP) clustering and GP regression to address the two main issues in Gaussian Process (GP) regression:

1) Data corrupted with noise generated from a non-Gaussian distribution.
2) The cubic complexity training time $\mathcal{O}(N^3)$.


The model automatically ignores data generated by noise sources when estimating the latent function as it uses an infinite mixture of Gaussian distributions to describe the noisy observations. In addition, it uses a distributed approach based on the product of robust GP experts to alleviate the computation memory and time.

![sine zoom](/figures/sine/sine_DDPGP.png?raw=true "Employee Data title")

<div align="center">Zoomed view</div>

![sine zoom](/figures/sine/sine_zoom.png?raw=true "Employee Data title")

# Development
The current version was developed on top of the `scikit-learn` packages `gaussian_process` and `mixture.BayesianGaussianMixture`.

Distributed computations were implemented using the Generalised Product of GPs detailed in the following [paper](https://proceedings.mlr.press/v37/deisenroth15.pdf).


# Example using synthetic data

Let us import some dependencies and simulate a toy dataset $\mathcal{D}$.

```python
import numpy as np
import matplotlib.pyplot as plt
from models.DGP import DistributedGP as DGP
from models.DDPGP import DistributedDPGP as DDPGP

# Generate always the same random numbers
from numpy.random import RandomState
prob = RandomState(123)

# Define the domain of the function and the number of observations
N = 2000
x = np.linspace(0, 10, N)

# Corrupt samples with noise generated from 2 independent sources
sine = lambda x: np.sin(x)
f = sine(x)
y = np.zeros(N)

# Noise structure
std1 = 0.1
std2 = 1
pi1 = 0.7
pi2 = 1 - pi1

# proportionalities vector
u = prob.uniform(0,1,N)

for i in range(N):
    if u[i] < pi1:
        y[i] = f[i] + std1*prob.randn()
    else:
        y[i] = f[i] + std2*prob.randn()

# define the covariance function
from sklearn.gaussian_process.kernels import RBF, WhiteKernel

se = 1**2 * RBF(length_scale=0.5, length_scale_bounds=(0.07, 0.9))
wn = WhiteKernel(noise_level=0.5**2, noise_level_bounds=(1e-6,0.7))

kernel = se + wn

# training
N_GPs = 5
model = DDPGP(x, y, N_GPs=N_GPs, init_K=7, kernel=kernel, normalise_y=True)
model.train()

# predictions
mu, std, betas = model.predict(x)
```
## Issues
If you find an error in your application, please raise it as an issue on the github page of the mian repository.

## Testing
We have developed some tests (`tests` folder) to validate the model on different scenarios. To run the test, first make sure you have `pytest` installed:

`python -m pip install pytest`

Then, in the command window, change to the Machine_Learning directory and simply type `pytest` in the command window. Note that these tests will be also be run automatically when somebody submits a pull request to the master branch.

## Next steps
Apply the DDPGP framework using the GPJax libraries to implement Sparse Robust GP experts.